{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55e8caed-5052-4d00-be32-d9fa549fab4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PixelObservationWrapper\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StepAPICompatibility\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms \u001b[38;5;28;01mas\u001b[39;00m T\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box\n",
    "from gymnasium.wrappers import FrameStack\n",
    "from gymnasium.wrappers import PixelObservationWrapper\n",
    "from gymnasium.wrappers import StepAPICompatibility\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, datetime, os\n",
    "from tensordict import TensorDict\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import gc\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import retro\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        for i in range(self._skip):\n",
    "            \n",
    "            obs, reward, done, trunk, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, trunk, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=240, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=240, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = T.Compose(\n",
    "            [T.Resize(self.shape, antialias=True), T.Normalize(0, 240)]\n",
    "        )\n",
    "        observation = transforms(observation).squeeze(0)\n",
    "        return observation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73d277ab-506c-4ca0-aa16-b01bfe6defee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<gymnasium.wrappers.frame_stack.LazyFrames at 0x76d3075966b0>, {})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_frames = 6\n",
    "env = retro.make(game='GhostsnGoblins-Nes', render_mode='human')\n",
    "env = SkipFrame(env, skip=stacked_frames) \n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=(67,72))\n",
    "\n",
    "if gym.__version__ < '0.26':\n",
    "    env = FrameStack(env, num_stack=stacked_frames, new_step_api=True)\n",
    "else:\n",
    "    env = FrameStack(env, num_stack=stacked_frames)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9961162-f631-44cb-ace3-6bc5fe3c74e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        c, h, w = input_dim\n",
    "        self.conv1 = nn.Conv2d(c, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(self._calculate_conv_output_size((c, h, w)), 512)\n",
    "        self.fc2 = nn.Linear(512, output_dim)\n",
    "\n",
    "    def _calculate_conv_output_size(self, shape):\n",
    "        c, h, w = shape\n",
    "        x = torch.randn(1, c, h, w)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        return int(np.prod(x.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8785640d-998c-4913-b499-41bc43e88dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, env, input_dim, output_dim, gamma=0.99, epsilon=1.0, epsilon_min=0.1, epsilon_decay=0.99999995, lr=0.001, replay_memory_size=100000, batch_size=32):\n",
    "        \"\"\"\n",
    "        Initieren von Hyperparameter\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.device =torch.device(\"cpu\")\n",
    "\n",
    "        \n",
    "        self.replay_memory = TensorDictReplayBuffer(storage=LazyMemmapStorage(replay_memory_size, device=torch.device(\"cpu\")))\n",
    "\n",
    "        self.model = DQN(input_dim, output_dim)\n",
    "        self.target_model = DQN(input_dim, output_dim)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def choose_action(self, state):\n",
    "\n",
    "        if isinstance(state, (int, float)): \n",
    "            state = torch.FloatTensor([state]).unsqueeze(0) \n",
    "        else:\n",
    "            state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n",
    "            state = torch.tensor(state, device=self.device).unsqueeze(0)  \n",
    "    \n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return self.env.action_space.sample()  \n",
    "        else:\n",
    "            q_values = self.model(state)\n",
    "            probs = F.softmax(q_values, dim=-1)\n",
    "            #print(probs)\n",
    "            threshold = 0.11 \n",
    "            #print(probs)\n",
    "            q_values = (probs > threshold).int() #entscheidung welche Erfahrungen wertvoll sind\n",
    "            action_index = q_values.argmax().item()\n",
    "            #print(q_values[action_index])\n",
    "            return q_values[action_index]\n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Store the experience in replay memory\n",
    "        \"\"\"\n",
    "        def first_if_tuple(x):\n",
    "            return x[0] if isinstance(x, tuple) else x\n",
    "        state = first_if_tuple(state).__array__()\n",
    "        next_state = first_if_tuple(next_state).__array__()\n",
    "\n",
    "        state = torch.tensor(np.array(state))\n",
    "        self.replay_memory.add(TensorDict({\n",
    "                \"state\": state,\n",
    "                \"action\": torch.tensor(np.array([action]), dtype=torch.int64, device=self.device),  # Convert to single NumPy array first\n",
    "                \"reward\": torch.tensor(np.array([reward]), dtype=torch.float32, device=self.device),\n",
    "                \"next_state\": next_state,\n",
    "                \"done\": torch.tensor(np.array([done]), dtype=torch.float32, device=self.device)\n",
    "        }, batch_size=[]))\n",
    "\n",
    "    def replay(self):\n",
    "     \n",
    "        if len(self.replay_memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        batch = self.replay_memory.sample(self.batch_size)#.to(self.device)\n",
    "        states = batch['state']\n",
    "        actions = batch['action'].squeeze().argmax(dim=1)\n",
    "        rewards = batch['reward'].squeeze()\n",
    "        next_states = batch['next_state']\n",
    "        dones = batch['done'].squeeze().float()\n",
    "\n",
    "        #current_q_values = self.model(states) #old code rev 1.0\n",
    "        current_q_values = self.model(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
    "        \n",
    "        next_q_values = self.target_model(next_states).max(1)[0].detach()\n",
    "        target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "       \n",
    "        #target_q_values = target_q_values.unsqueeze(1) # old code rev 1.0\n",
    "        #current_q_values = current_q_values.unsqueeze(1) # old code rev 1.0\n",
    "\n",
    "        target_q_values = target_q_values.view(-1)  # Flatten target_q_values to match current_q_values shape\n",
    "        #current_q_values = current_q_values.view(-1)\n",
    "        \n",
    "        loss = self.loss_fn(current_q_values, target_q_values.unsqueeze(1))\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def train(self, episodes=10000, max_steps_per_episode=1000000):\n",
    "        scores = []\n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            total_reward = 0\n",
    "            score = 0\n",
    "            time = 0\n",
    "            for step in range(max_steps_per_episode):\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done, truncated, info = self.env.step(action)\n",
    "                if info[\"score\"] % 200 == 0 and score != info[\"score\"]:\n",
    "                    reward += 0.1\n",
    "                    score = info[\"score\"]\n",
    "                total_seconds = info[\"time_minutes\"] * 60 + info[\"time_tens\"] * 10 + info[\"time_ones\"]\n",
    "                if time > total_seconds:\n",
    "                    reward += 0.025\n",
    "                    time = total_seconds\n",
    "\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "\n",
    "                self.replay()\n",
    "\n",
    "                total_reward += reward\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            scores.append(total_reward)\n",
    "            print(f\"Episode {episode + 1}/{episodes}, Score: {total_reward}, Epsilon: {self.epsilon:.2f}\")\n",
    "            if (episode + 1) % 10 == 0:\n",
    "                self.update_target_model()\n",
    "\n",
    "        print('Training complete.')\n",
    "\n",
    "        # Plotting the rewards graph\n",
    "        plt.plot(scores)\n",
    "        plt.xlabel('Episodes')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.title('Training Rewards')\n",
    "        plt.grid(True)\n",
    "        plt.savefig('training_rewards.png')  # Save the plot as an image file\n",
    "        plt.show()\n",
    "\n",
    "        # Save the trained model\n",
    "        torch.save(self.model.state_dict(), 'trained_model.pth')\n",
    "        \n",
    "        return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fa8bc67-9edb-4159-a412-bc0c6db283c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_saliency(model, state, action):\n",
    "    state = torch.tensor(state, requires_grad=True)\n",
    "    model.eval()\n",
    "    q_values = model(state)\n",
    "    q_value = q_values[0, action]\n",
    "    q_value.backward()\n",
    "    saliency, _ = torch.max(state.grad.data.abs(), dim=1)\n",
    "    return saliency\n",
    "\n",
    "# Visualize the saliency map\n",
    "def visualize_saliency(state, saliency):\n",
    "    state = state.squeeze().detach().numpy()\n",
    "    saliency = saliency.squeeze().detach().numpy()\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title('State')\n",
    "    plt.imshow(state, cmap='gray')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title('Saliency')\n",
    "    plt.imshow(saliency, cmap='hot')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9047e286-b5bb-46a6-b44d-732653d458dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stacked_frames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m input_dim \u001b[38;5;241m=\u001b[39m (\u001b[43mstacked_frames\u001b[49m, \u001b[38;5;241m67\u001b[39m, \u001b[38;5;241m72\u001b[39m) \n\u001b[1;32m      3\u001b[0m output_dim \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn \n\u001b[1;32m      4\u001b[0m agent \u001b[38;5;241m=\u001b[39m DQNAgent(env, input_dim, output_dim)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stacked_frames' is not defined"
     ]
    }
   ],
   "source": [
    "input_dim = (stacked_frames, 67, 72) \n",
    "\n",
    "output_dim = env.action_space.n \n",
    "agent = DQNAgent(env, input_dim, output_dim)\n",
    "scores = agent.train()\n",
    "\n",
    "\n",
    "state = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = agent.choose_action(state)\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "    saliency = generate_saliency(model, state_tensor, action)\n",
    "    visualize_saliency(state, saliency)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    state = next_state\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e945c78-5dee-44af-8e35-2be28f7b0246",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc0525f-8e8e-41b3-b4be-d68299593f26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
